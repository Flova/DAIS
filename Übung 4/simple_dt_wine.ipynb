{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "# Import everything you need\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  94.44 %\n"
     ]
    }
   ],
   "source": [
    "#### Load the data ... \n",
    "dataset = load_wine()\n",
    "\n",
    "# Split it in train and test set (mind the value for test_size and random_state- how does they affect your model?)\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.40, random_state=42)\n",
    "\n",
    "# Initialize a model and train it (yes, it is actually just the *.fit() call)\n",
    "dt_wine = tree.DecisionTreeClassifier(criterion=\"gini\")\n",
    "dt_wine = dt_wine.fit(X_train,y_train)\n",
    "\n",
    "# Compute the predictions with your trained model\n",
    "y_pred = dt_wine.predict(X_test)\n",
    "\n",
    "# Get a first impression: How did it work?\n",
    "test_accuracy= metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", '%.2f'% (test_accuracy*100),\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23  3  0]\n",
      " [ 0 27  0]\n",
      " [ 1  0 18]]\n"
     ]
    }
   ],
   "source": [
    "# You almost never get 100% accuracy; to check where the error may come from use a confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.81      0.96      0.88        26\n",
      "     class_1       0.81      0.78      0.79        27\n",
      "     class_2       0.93      0.74      0.82        19\n",
      "\n",
      "    accuracy                           0.83        72\n",
      "   macro avg       0.85      0.83      0.83        72\n",
      "weighted avg       0.84      0.83      0.83        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can calculate true positive etc. from the confusion matrix....or use scikit to do it for you. What is the difference\n",
    "# between precision and recall (do not mind micro avg. etc.)\n",
    "\n",
    "print(classification_report(y_test, y_pred, \n",
    "                   target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You think the model could do better? You called the decision tree with default values; anything you would like to change?\n",
    "# If so, train the model with a different configuration and test it again. Will your evaluation metric change? If so, how?\n",
    "\n",
    "tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate with crossvalidation\n",
    "\n",
    "#### Load the data ... \n",
    "dataset = load_wine()\n",
    "\n",
    "# So just import it instead of 'train_test_split'\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Get some data, use X for the datapoints and y for the label (or rename but then change accordingly)\n",
    "\n",
    "# Define a classifier\n",
    "\n",
    "dt_wine = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Here, the k-fold crossvalidation starts with k=10; you do not pass separated train and test sets anymore \n",
    "scores = cross_val_score(dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88888889 0.88888889 0.66666667 0.88888889 0.83333333 0.83333333\n",
      " 1.         0.94444444 0.94117647 0.76470588]\n",
      "0.865032679738562 0.0913519991478125\n"
     ]
    }
   ],
   "source": [
    "# integrate with crossvalidation\n",
    "\n",
    "#### Load the data ... \n",
    "dataset = load_wine()\n",
    "\n",
    "# So just import it instead of 'train_test_split'\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Get some data, use X for the datapoints and y for the label (or rename but then change accordingly)\n",
    "\n",
    "# Define a classifier\n",
    "\n",
    "dt_wine = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Here, the k-fold crossvalidation starts with k=10; you do not pass separated train and test sets anymore \n",
    "scores = cross_val_score(dt_wine, dataset.data, dataset.target, cv=10, scoring='accuracy')\n",
    "# Prints all scores of all folds\n",
    "print(scores)\n",
    "\n",
    "# Prints the average, this is actually what you want and report in a paper\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_state': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], 'criterion': ['gini', 'entropy']}\n",
      "[0.87058824 0.86503268 0.87614379 0.86503268 0.85947712 0.85947712\n",
      " 0.85947712 0.87058824 0.87058824 0.85947712 0.85392157 0.88169935\n",
      " 0.85947712 0.87058824 0.85947712 0.88169935 0.86503268 0.87614379\n",
      " 0.85947712 0.85947712 0.86503268 0.8872549  0.86503268 0.86503268\n",
      " 0.87058824 0.87058824 0.86503268 0.87614379 0.85359477 0.86503268\n",
      " 0.85392157 0.87058824 0.86503268 0.85392157 0.85359477 0.85359477\n",
      " 0.87026144 0.87058824 0.87026144 0.88169935 0.86470588 0.87058824\n",
      " 0.86503268 0.85915033 0.87058824 0.87058824 0.85947712 0.86470588\n",
      " 0.85915033 0.87026144 0.89869281 0.89869281 0.92091503 0.8869281\n",
      " 0.8875817  0.89313725 0.8872549  0.90424837 0.90424837 0.90424837\n",
      " 0.89869281 0.89869281 0.89313725 0.89836601 0.89313725 0.89313725\n",
      " 0.91535948 0.90424837 0.90980392 0.89869281 0.89869281 0.90424837\n",
      " 0.89248366 0.91535948 0.90980392 0.89803922 0.8875817  0.92091503\n",
      " 0.91535948 0.89281046 0.89869281 0.89313725 0.89869281 0.89248366\n",
      " 0.89869281 0.92091503 0.90980392 0.88202614 0.89281046 0.89836601\n",
      " 0.90424837 0.90424837 0.91535948 0.89869281 0.89313725 0.90424837\n",
      " 0.90424837 0.89313725 0.89248366 0.89836601]\n",
      "[0.08011803 0.091352   0.09355142 0.07665541 0.1010433  0.10405303\n",
      " 0.09473743 0.08011803 0.07616833 0.1010433  0.10674875 0.06964389\n",
      " 0.10405303 0.07616833 0.08798075 0.06964389 0.07665541 0.07926369\n",
      " 0.10405303 0.1010433  0.091352   0.07216601 0.07665541 0.07665541\n",
      " 0.06757993 0.08011803 0.0879085  0.07926369 0.09574438 0.10399194\n",
      " 0.10080044 0.09094367 0.07665541 0.10674875 0.09891547 0.09891547\n",
      " 0.08167385 0.08011803 0.08891111 0.0818665  0.08930841 0.06757993\n",
      " 0.091352   0.09601894 0.06757993 0.07616833 0.09473743 0.08578291\n",
      " 0.09918126 0.08167385 0.10346634 0.08728611 0.07385621 0.0878374\n",
      " 0.09127188 0.08947388 0.08026519 0.088248   0.07292855 0.07704452\n",
      " 0.07594085 0.08728611 0.08947388 0.08712016 0.09928457 0.08947388\n",
      " 0.07396675 0.088248   0.0922611  0.09409268 0.10043904 0.07704452\n",
      " 0.08600982 0.07396675 0.0922611  0.0718776  0.09459303 0.07385621\n",
      " 0.07396675 0.08208796 0.10043904 0.08947388 0.08728611 0.08234321\n",
      " 0.09409268 0.07385621 0.0922611  0.10220402 0.07823778 0.06233955\n",
      " 0.088248   0.088248   0.07396675 0.08728611 0.08947388 0.09167875\n",
      " 0.088248   0.08947388 0.06077816 0.08350233]\n",
      "0.9209150326797385 {'criterion': 'entropy', 'random_state': 2}\n"
     ]
    }
   ],
   "source": [
    "# integrate with grid search\n",
    "\n",
    "'''An even easier approach (and even more \"black box\") can be achieved using grid search. \n",
    "*Adapt* this code for the integration into your experiments'''\n",
    "\n",
    "# Import gridsearch with crossvalidation \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Get some data; please use X for the datapoints and y for the labels, or rename and change it accordingly\n",
    "\n",
    "# create a parameter grid: map the parameter names to the values that should be searched\n",
    "param_grid = dict(random_state=list(range(0, 50)), criterion=[\"gini\", \"entropy\"])\n",
    "print(param_grid)\n",
    "\n",
    "dt_wine = tree.DecisionTreeClassifier()\n",
    "# Setup the grid with your classifier and the default cross-validation value k=10\n",
    "grid = GridSearchCV(dt_wine, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "# ... and train it as before\n",
    "grid.fit(dataset.data, dataset.target)\n",
    "\n",
    "# Get the mean accuracy over all folds\n",
    "grid_mean_scores = grid.cv_results_['mean_test_score']\n",
    "print(grid_mean_scores)\n",
    "\n",
    "# It is also important to get the standard deviation per fold \n",
    "grid_std_score= grid.cv_results_['std_test_score']\n",
    "print(grid_std_score)\n",
    "\n",
    "# Even more high-level, we can just use 'pandas' to print it nicer\n",
    "import pandas as pd\n",
    "pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']]\n",
    "\n",
    "# But usually, we are interested in the best fit, which returns the best achieved accuracy over all folds and the value\n",
    "# for the parameter under investigation\n",
    "\n",
    "print(grid.best_score_, grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
